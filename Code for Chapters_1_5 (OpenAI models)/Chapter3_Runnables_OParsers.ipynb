{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chapter 3\n",
    "\n",
    "    - This notebook contains the code for Chapter 3 of the book."
   ],
   "id": "2448e3d63e77ecb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:11:49.326924Z",
     "start_time": "2025-07-17T11:11:37.737891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "OPENAI_API_KEY = getpass()"
   ],
   "id": "32380523c94b4684",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:11:51.566457Z",
     "start_time": "2025-07-17T11:11:51.562797Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY",
   "id": "a7598223653b1a51",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.1a Wrapping RunnableLambda around a Python function",
   "id": "93305f425398af2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:11:22.983116Z",
     "start_time": "2025-07-17T11:11:22.290752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "import re\n",
    "\n",
    "# Step 1: Define a preprocessing function to sanitize input\n",
    "def sanitize_text(text):\n",
    "    censored_list = [\"shit\", \"damn\", \"hell\"]\n",
    "    for bad_word in censored_list:\n",
    "        text = re.sub(rf\"\\b{bad_word}\\b\", \"[censored]\", text, flags = re.IGNORECASE)\n",
    "    return text.strip()\n",
    " \n",
    "# Step 2: Wrap this function in RunnableLambda\n",
    "sanitize_text_runnable = RunnableLambda(sanitize_text)\n",
    " \n",
    "# Stape 3: Invoke it\n",
    "sanitize_text_runnable.invoke(\"What is this damn response?\")\n"
   ],
   "id": "ade8b071ecde805c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is this [censored] response?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.1b Building a chain with RunnableLambda and other components (OpenAI model)",
   "id": "2b96554d79be3304"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:11:59.609950Z",
     "start_time": "2025-07-17T11:11:56.853628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import re\n",
    " \n",
    "## Step 1: Define a custom function\n",
    "def sanitize_text(text):\n",
    "    censored_list = [\"shit\", \"damn\", \"hell\"]\n",
    "    for bad_word in censored_list:\n",
    "        text = re.sub(rf\"\\b{bad_word}\\b\", \"[censored]\", text, flags = re.IGNORECASE)\n",
    "    return text.strip()\n",
    " \n",
    "## Step 2: Wrap it in RunnableLambda\n",
    "sanitize_text_runnable = RunnableLambda(sanitize_text)\n",
    " \n",
    "## Step 3: Define the template string for the ChatPromptTemplate\n",
    "messages =[\n",
    "    \n",
    "    (\"system\",\"You are a helpful assistant for school administration.\"),\n",
    "    \n",
    "    (\"human\", \"How much is the annual tuition fee for Grade 8?\"),\n",
    "    (\"ai\", \"The annual tuition fee for Grade 8 is $4,200. You can pay in three installments.\"),\n",
    "    (\"human\", \"What is [censored]?\"),\n",
    "    (\"ai\", \"Part of your question/statement has been censored question/statement, kindly rephrase without profanity\"),\n",
    " \n",
    "    (\"human\", \"{user_question}\")\n",
    "] \n",
    " \n",
    "## Step 4: Link the message with the template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "## Step 5: Define the model and the llm chat\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                 model = \"gpt-4o-mini\",\n",
    "                 temperature = 0)\n",
    " \n",
    "## Step 6: Define the chain\n",
    "chain = {\"user_question\":sanitize_text_runnable} | chat_template | llm\n",
    " \n",
    "## Step 7: Invoke the chain\n",
    "response = chain.invoke(\"What do you know about the damn annual tuition fee?\")\n",
    " \n",
    "## Print the response\n",
    "print(response.content)\n"
   ],
   "id": "76267ec1e356ec87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that part of your question has been censored. If you could provide more context or clarify what specific information you are looking for regarding the annual tuition fee, I would be happy to assist you!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.2 Building a chain with RunnableSequence and other components (OpenAI model)",
   "id": "c97818c31ff0537c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 1 – 3:",
   "id": "8e94ede48874f00a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T03:20:10.094737Z",
     "start_time": "2025-07-14T03:20:10.081680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "import re\n",
    " \n",
    "## Step 1: Define a custom function for sanitization\n",
    "def sanitize_text(text):\n",
    "    censored_list = [\"shit\", \"damn\", \"hell\"]\n",
    "    for bad_word in censored_list:\n",
    "        text = re.sub(rf\"\\b{bad_word}\", \"[censored]\", text, flags = re.IGNORECASE)\n",
    "    return text.strip()\n",
    " \n",
    "## Step 2: Define a custom function for validation of input\n",
    "def validate_question(input: dict)->dict:\n",
    "    q = input.get(\"user_question\", \" \").lower()\n",
    "    school_keywords = [\"fee\", \"holiday\", \"school\"]\n",
    "    if not any(keyword in q for keyword in school_keywords):\n",
    "        raise ValueError(\"Sorry, I can only assist with school-related questions.\")\n",
    "    return {\"validated_question\": input[\"user_question\"]}\n",
    " \n",
    "## Step 3: Transform the custom functions into runnables\n",
    "sanitize_text_runnable = RunnableLambda(sanitize_text)\n",
    "validate_runnable = RunnableLambda(validate_question) "
   ],
   "id": "3f8f0f3cd776b140",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Steps 4 – 5:"
   ],
   "id": "70ddaaa109c443c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T03:20:17.077914Z",
     "start_time": "2025-07-14T03:20:17.069964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 4: Define the template string for the ChatPromptTemplate\n",
    "messages =[\n",
    "    \n",
    "    (\"system\",\"You are a helpful assistant for school administration.\"),\n",
    "    \n",
    "    (\"human\", \"How much is the annual tuition fee for Grade 8?\"),\n",
    "    (\"ai\", \"The annual tuition fee for Grade 8 is $4,200. You can pay in three installments.\"),\n",
    "    (\"human\", \"What is [censored]?\"),\n",
    "    (\"ai\", \"Part of your question/statement has been censored question/statement, rephrase without profanity\"),\n",
    " \n",
    "    (\"human\", \"{validated_question}\")\n",
    "]\n",
    " \n",
    "## Step 5: Link the message with the template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    messages=messages\n",
    ") "
   ],
   "id": "ef43e05f32eabb9b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "##### Steps 6 – 9:"
   ],
   "id": "bebd6b55c51ea28a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T03:20:29.081406Z",
     "start_time": "2025-07-14T03:20:26.753540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 6: Define the model and the llm chat\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                 model = \"gpt-4o-mini\",\n",
    "                 temperature = 0,\n",
    "                 max_tokens = 100)\n",
    " \n",
    " \n",
    "## Step 7: Define the input list\n",
    "question_list =[\n",
    "    \"Is there a holiday coming up that affects school schedules?\",\n",
    "    \"What do you know about the damn annual tuition fee?\",\n",
    "    \"Tell me more about the annual tuition fee.\"\n",
    "    \n",
    "]\n",
    "\n",
    " \n",
    "## Step 8: Compose the chain using the RunnableSequence\n",
    "full_chain = RunnableSequence({\"user_question\":sanitize_text_runnable},\n",
    "                              {\"validated_question\":validate_runnable},\n",
    "                              chat_template,\n",
    "                              llm)\n",
    " \n",
    "# Step 9: Call the .batch method\n",
    "response = full_chain.batch(question_list)\n"
   ],
   "id": "30f60f5ca4cd5a17",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T03:20:50.265750Z",
     "start_time": "2025-07-14T03:20:50.260044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response[0].content)\n",
    "print(\"\\n\")\n",
    "print(response[1].content)\n",
    "print(\"\\n\")\n",
    "print(response[2].content)\n"
   ],
   "id": "d4b19c48aafa4c64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are upcoming holidays that may affect school schedules. Please check the school calendar for specific dates and any related schedule changes. If you need information about a particular holiday, feel free to ask!\n",
      "\n",
      "\n",
      "It seems that part of your question has been censored. If you are asking about the annual tuition fee for a specific program or school, please provide the name of the school or program, and I will do my best to assist you with the information you need.\n",
      "\n",
      "\n",
      "The annual tuition fee for Grade 8 is $4,200. This fee typically covers various educational expenses, including classroom materials, access to facilities, and extracurricular activities. The tuition can usually be paid in three installments throughout the academic year. If you have any specific questions about what the tuition includes or payment options, feel free to ask!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.3a – Setting up the LLM-as-judge with procedure",
   "id": "6c21ea8edb6d42c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 1-3",
   "id": "1bee51895dab5572"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T22:00:16.180465Z",
     "start_time": "2025-07-14T22:00:16.169698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary modules\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnableSequence\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    " \n",
    "## Step 1:Define template string\n",
    "## Take note of the variable we introduced here\n",
    "messages = [\n",
    "        (\"system\", \"You are a helpful assistant for school administration.\"),\n",
    "        (\"human\", \"Explain in layman the topic: {topic}\"),  \n",
    "        ]\n",
    "\n",
    "## Step 2a:Connect the template string to the chat prompt template\n",
    "llm1_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=messages\n",
    "        \n",
    ")\n",
    "\n",
    "## Step 2b:Connect the template string to the chat prompt template\n",
    "llm2_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=messages\n",
    ")\n",
    " \n",
    "## Step 3:Define prompt for the judge model to compare LLM1 vs LLM2\n",
    "judge_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages = [\n",
    "        (\"system\", \"You are to act as a judge to determine the simplicity of language between two responses.\"),\n",
    "        (\"human\", \"\"\"Don't re-define and don't add anything, just compare in terms \n",
    "        of simplicity: {llm1_response} or {llm2_response}?\"\"\"),\n",
    "        ]\n",
    ") \n",
    " \n"
   ],
   "id": "b6adfbc6a0e7893e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 4-7:",
   "id": "771c353644a6be70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T22:00:27.022117Z",
     "start_time": "2025-07-14T22:00:23.583056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 4:Initialize the LLMs and assign models\n",
    "model1 = ChatOpenAI(api_key=OPENAI_API_KEY, model = \"gpt-4o-mini\", max_tokens = 90)\n",
    "model2 = ChatOpenAI(api_key=OPENAI_API_KEY, model = \"gpt-4.1-nano\", max_tokens = 90)\n",
    "judge_llm = ChatOpenAI(api_key=OPENAI_API_KEY, model = \"gpt-4o-mini\", max_tokens = 90)\n",
    " \n",
    "## Step 5:Bind templates to models to create runnable LLM chains\n",
    "llm1_chain = llm1_prompt| model1 \n",
    "llm2_chain = llm2_prompt | model2\n",
    "llm_judge_chain = judge_prompt | judge_llm\n",
    " \n",
    " \n",
    "## Step 6:Identity passthrough to forward topic along the chain\n",
    "identity_chain = RunnableLambda(lambda x: x['topic'])\n",
    " \n",
    "## Step 7: Branch out input to run both LLMs and retain original topic\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"llm1\": llm1_chain,\n",
    "    \"llm2\": llm2_chain,\n",
    "    \"topic\": identity_chain}\n",
    ")\n",
    " \n"
   ],
   "id": "e75255db11c6e9d6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 8-9",
   "id": "b7e08272db13d5ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T22:00:29.354891Z",
     "start_time": "2025-07-14T22:00:29.349865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 8: Let’s temporarily invoke the parallel chain to observe\n",
    "response = parallel_chain.invoke({\"topic\": \"What does the grading system mean?\"}) \n",
    "\n",
    "\n",
    "## The output is a dictionary. So, we extract as:\n",
    "print(response[\"llm1\"].content)\n",
    "print(\"\\n\")\n",
    "print(response[\"llm2\"].content)\n",
    "print(\"\\n\")\n",
    "print(response[\"topic\"])\n"
   ],
   "id": "9711a3495b9f532e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading system is a framework used by educational institutions to assess and communicate students' performance and understanding of course material. Typically, grades are assigned based on a combination of assignments, tests, participation, and overall engagement. Common grading scales include letter grades (A, B, C, etc.), numerical scores (0-100), or pass/fail options. These grades serve multiple purposes: they reflect a student's academic achievement, provide feedback for improvement, and\n",
      "\n",
      "\n",
      "The grading system is a method used by educational institutions to evaluate and represent students' academic performance. It assigns letter grades, numbers, or descriptive levels to indicate how well a student has mastered the material. The system helps communicate achievement levels, determine eligibility for advancement or graduation, and provide feedback for improvement. Different institutions may use varying grading scales, such as A-F, percentage scores, or other symbols, but their primary purpose is to quantify student learning outcomes\n",
      "\n",
      "\n",
      "What does the grading system mean?\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NOTE: \n",
    "    \n",
    "    >\n",
    "    \n",
    "        Before moving to the next recipe, note that it is also possible to write the line for the RunnableParallel (in Step 8), not as a dictionary, but as:"
   ],
   "id": "34b6ad3ae68f6327"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T00:46:26.275070Z",
     "start_time": "2025-07-15T00:46:21.187380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "parallel_chain2 = RunnableParallel(llm1 = llm1_chain,\n",
    "                                  llm2 = llm2_chain,\n",
    "                                  topic = RunnablePassthrough()\n",
    ")\n",
    "\n",
    "\n",
    "## Step 8: We invoke the chain\n",
    "response2 = parallel_chain2.invoke(\"What does the grading system mean?\") \n",
    "\n",
    "## The output is a dictionary. So, we extract as:\n",
    "print(response2[\"llm1\"].content)\n",
    "print(\"\\n\")\n",
    "print(response2[\"llm2\"].content)\n",
    "print(\"\\n\")\n",
    "print(response2[\"topic\"])"
   ],
   "id": "33d96fc0e2db88d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading system is a standardized method used to evaluate and communicate a student's academic performance. It typically includes a range of letter grades (A, B, C, D, F) or numerical scores (0-100), with each grade reflecting a student's level of understanding and mastery of the material. \n",
      "\n",
      "Grades are often associated with specific criteria, such as assignments, exams, and participation, and they can influence student progression, eligibility for programs, and GPA\n",
      "\n",
      "\n",
      "The grading system is a method used by educational institutions to evaluate and represent a student's academic performance. It typically assigns letter grades, percentages, or scores to indicate how well a student has mastered the material. These grades help communicate a student's progress, determine eligibility for advancement, and provide feedback for improvement. Different schools may have varying grading scales, but the overall purpose is to assess learning outcomes consistently.\n",
      "\n",
      "\n",
      "What does the grading system mean?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Recipe 3.1.3b – Completing the LLM-as-judge procedure\n",
    "\n",
    "NOTE:\n",
    "    \n",
    "    > The code in this section should be combined with the evaluations of Steps 1-9 above."
   ],
   "id": "967f40e8899b81c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T04:03:37.812540Z",
     "start_time": "2025-07-15T04:03:34.395761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 9: Evaluate and compare LLM1 vs LLM2 using a critic model\n",
    "# The evaluate function processes the outputs coming out of the final parallel chain\n",
    "def evaluate(parallel_output):\n",
    "    topic = parallel_output[\"topic\"]\n",
    "    llm1_response = parallel_output[\"llm1\"].content\n",
    "    llm2_response = parallel_output[\"llm2\"].content\n",
    "    judge_list = [{\"llm1_response\":llm1_response, \"llm2_response\":llm2_response}]\n",
    "    ## Call the .batch method to process \n",
    "    judgement = llm_judge_chain.batch(judge_list)[0].content        \n",
    "    ### recall that .batch() method returns a list ()\n",
    "    return {\"llm1_response\":llm1_response, \n",
    "            \"llm2_response\":llm2_response, \n",
    "            \"evaluation\": judgement}\n",
    " \n",
    "## Step 10: Wrap the evaluator as a runnable\n",
    "evaluation_chain = RunnableLambda(evaluate)\n",
    " \n",
    "## Step 11: Compose the full pipeline: generate explanations \n",
    "full_chain = RunnableSequence(parallel_chain, evaluation_chain)\n",
    " \n",
    "## Step 12: Run the full pipeline with a topic prompt\n",
    "output = full_chain.invoke({\"topic\": \"What does the grading system mean?\"})\n",
    "\n",
    "\n",
    "## Print the output\n",
    "print(f\"LLM 1 response: {output[\"llm1_response\"]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"LLM 2 response: {output[\"llm2_response\"]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The verdict:{output[\"evaluation\"]}\")\n"
   ],
   "id": "2cd7b1fb1f95b517",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 1 response: The grading system is a method used to evaluate and communicate a student's academic performance and progress. It typically uses letters (A, B, C, D, F) or percentages to indicate levels of achievement in a subject or course. \n",
      "\n",
      "- **Letter Grades**: Commonly, an 'A' represents excellent performance, 'B' good performance, 'C' average, 'D' below average, and 'F' failing.\n",
      "  \n",
      "- **\n",
      "\n",
      "\n",
      "LLM 2 response: The grading system is a method used by schools to evaluate and represent students' academic performance. It assigns letter grades or numerical scores based on how well students meet learning objectives, helping educators, students, and parents understand progress and areas needing improvement.\n",
      "\n",
      "\n",
      "The verdict:The second response is simpler. It uses more straightforward language and avoids complex phrases, making it easier to understand. The first response contains more formal terminology and additional details that could make it less accessible.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.4 – Building a book recommending pipeline using RunnablePassthrough and RunnableLambda",
   "id": "91a9f3860136c2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T03:58:38.859034Z",
     "start_time": "2025-07-16T03:58:36.720171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Import necessary modules from Langchain\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "## Step 1: Define a function to get a list of recommended books\n",
    "def get_recommended_books():\n",
    "    return [\n",
    "        \"To Kill a Mockingbird by Harper Lee\",\n",
    "        \"1984 by George Orwell\",\n",
    "        \"Of Mice and Men by John Steinbeck\",\n",
    "        \"Animal Farm by George Orwell\"\n",
    "    ]\n",
    " \n",
    "## Step 2: Create a RunnableLambda to execute the get_recommended_books function\n",
    "books_runnable = RunnableLambda(lambda _: get_recommended_books())\n",
    "#books_runnable = RunnableLambda(get_recommended_books)\n",
    " \n",
    "## Step 3: Define a prompt template to format the input for the LLM\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"Select {n} classic books recommended for secondary school students:\\n{books}\"\n",
    ")\n",
    " \n",
    "## Step 4: Initialize the ChatOpenAI model (replace with your actual API key)\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    " \n",
    "## Step 5: Create a runnable chain that passes the input 'n' \n",
    "## and the output of 'books_runnable' to the prompt, # then to the LLM\n",
    "book_chain = {\"n\": RunnablePassthrough(), \"books\": books_runnable} | prompt_template | llm\n",
    " \n",
    "## Step 6: Invoke the chain\n",
    "output = book_chain.invoke({\"n\": 2})\n",
    "print(output.content) \n"
   ],
   "id": "19ec4102a8743db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 2 classic books recommended for secondary school students:\n",
      "\n",
      "1. **To Kill a Mockingbird by Harper Lee**\n",
      "2. **1984 by George Orwell**\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.1.5 Dynamic routing with RunnableBranch ",
   "id": "a26a55019bce37a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 1 - 3",
   "id": "d6cf53da953d9aff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:33:26.266423Z",
     "start_time": "2025-07-16T07:33:26.163109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "# Step 1: Define example question-answer pairs for admissions-related queries.\n",
    "examples_qa_admissions =[\n",
    "    {\"question\": \"How do I update my contact information in my application\", \n",
    "     \"answer\": \"Log into your applicant portal, go to 'Personal Information' section to edit your details.\"},\n",
    "    \n",
    "    {\"question\": \"My child is waitlisted. When will we know if they're accepted?\", \n",
    "     \"answer\": \"Waitlist decisions come by June 15th via email and portal.\"}\n",
    "]\n",
    " \n",
    "# Step 2: Define example question-answer pairs for exams-related queries.\n",
    "examples_qa_exams =[\n",
    "    {\"question\": \"How can my child reschedule an exam due to a medical appointment?\",\n",
    "     \"answer\": \"Kindly, submit the 'Exam Absence Request Form' from the school portal.\"},\n",
    "    \n",
    "    {\"question\": \"Where can I find the midterm exam schedule?\",\n",
    "     \"answer\": \"Access the schedule on the school website under 'Academic Calendar'.\"}\n",
    "]\n",
    " \n"
   ],
   "id": "55b88db12fbea63c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 3 - 5",
   "id": "af4a4d218308af8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:33:40.429763Z",
     "start_time": "2025-07-16T07:33:40.420329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Create a prompt template for a single question-answer pair.\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables = [\"question\", \"answer\"],\n",
    "    template = \"Question: {question}\\n Answer: {answer}\"\n",
    ")\n",
    " \n",
    "# Step 4: Create a FewShotPromptTemplate for admissions questions, using the defined examples and prompt template above\n",
    "few_shot_prompt_admissions = FewShotPromptTemplate(\n",
    "    examples = examples_qa_admissions,\n",
    "    example_prompt = example_prompt,\n",
    "    input_variables=[\"user_question\"],\n",
    "    suffix=\"Q: {user_question}\\nA:\",\n",
    "    prefix=\"\"\"You are a helpful administrative assistant. \n",
    "    Below are examples of question-answer pairs for your guide.\"\"\",\n",
    ")\n",
    " \n",
    "# Step 5: Create a FewShotPromptTemplate for exams questions, using the defined examples and prompt.\n",
    "few_shot_prompt_exams = FewShotPromptTemplate(\n",
    "    examples = examples_qa_exams,\n",
    "    example_prompt = example_prompt,\n",
    "    input_variables=[\"user_question\"],\n",
    "    suffix=\"Q: {user_question}\\nA:\",\n",
    "    prefix=\"\"\"You are a helpful administrative assistant. \n",
    "    Below are examples of question-answer pairs for your guide.\"\"\",\n",
    ")\n",
    " \n"
   ],
   "id": "19567ca683cac2ca",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 6 – 8",
   "id": "b728d9d8fe3a708"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:39:09.392960Z",
     "start_time": "2025-07-16T07:39:08.888617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Define the llm to be used.\n",
    "llm = ChatOpenAI(\n",
    "    model= \"gpt-4o-mini\",\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")\n",
    " \n",
    "# Step 7: Create the respective chain by combining the admissions prompt with the LLM.\n",
    "admissions_chain = few_shot_prompt_admissions | llm\n",
    " \n",
    "# Step 8: Create a chain by combining the exams prompt with the LLM.\n",
    "exams_chain = few_shot_prompt_exams | llm\n"
   ],
   "id": "c74ee38b16a9f6af",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Steps 9 - 11",
   "id": "cda14fab9eec71ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:57:39.585973Z",
     "start_time": "2025-07-16T07:57:39.577311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 9: Define a function to route the user's question to the appropriate topic.\n",
    "def route_topic(question: str):\n",
    "    q = question.lower()\n",
    "    if \"admission\" in q or \"waitlist\" in q or \"application\" in q:\n",
    "        return \"admissions\"\n",
    "    elif \"exam\" in q or \"schedule\" in q or \"reschedule\" in q:\n",
    "        return \"exams\"\n",
    "\n",
    " \n",
    "# Step 10: Define a fallback runnable to handle questions that don't match any specific topic.\n",
    "default_runnable = RunnableLambda(lambda x: \"I'm not sure how to answer that yet.\")\n",
    " \n",
    "# Step 11: Create a RunnableBranch to route the input based on the output of the route_topic function.\n",
    "# The branch receives a dictionary as input\n",
    "# Note: in the branch below, x represents the input dictionary containing user_question as a key\n",
    "qa_router = RunnableBranch(\n",
    "    (lambda x: route_topic(x[\"user_question\"]) == \"admissions\", admissions_chain),\n",
    "    (lambda x: route_topic(x[\"user_question\"]) == \"exams\", exams_chain),\n",
    "    default_runnable \n",
    ")\n"
   ],
   "id": "9b1c73de6161ff71",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T07:57:43.809161Z",
     "start_time": "2025-07-16T07:57:42.460107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "Q1 = qa_router.invoke({\"user_question\": \"My child is waitlisted. When will we know if they're accepted?\"})\n",
    "#Q2 = qa_router.invoke({\"user_question\": \"Where can I find the midterm exam schedule?\"})\n",
    " \n",
    "## Print the response\n",
    "print(Q1.content)\n",
    "#print(Q2.content)\n"
   ],
   "id": "593484fb3ca0e268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waitlist decisions are typically communicated by June 15th via email and on the portal.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.2.1a String output parser with OpenAI model",
   "id": "c062340a259173bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T08:53:43.315235Z",
     "start_time": "2025-07-16T08:53:39.181016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    " \n",
    "## Step 1: Define a model\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                        model = \"gpt-4o-mini\")\n",
    " \n",
    "## Step 2: Define a template\n",
    "template_string = PromptTemplate.from_template(\n",
    "    inpput_variables = [\"topic\"],\n",
    "    template = \"\"\"\n",
    "    Explain the topic provided below in 3 bullet points.\n",
    "    Each bullet point should be concise and only be 1 sentence long.\n",
    "    Topic: {topic}\n",
    "    \"\"\"\n",
    ")\n",
    " \n",
    "## Step 3: Establish a chain with no parser\n",
    "chain_without_parser = template_string | chat_model\n",
    " \n",
    "## Step 4: Establish a chain with parser\n",
    "chain_with_parser = template_string | chat_model | StrOutputParser()\n",
    "\n",
    "## Step 5: Invoking chain with no output parser\n",
    "response_1 = chain_without_parser.invoke({\"topic\": \"Learning objectives\"})\n",
    "print(response_1)\n",
    " \n"
   ],
   "id": "206348445bb2f322",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='- Learning objectives clearly define the specific skills and knowledge that learners are expected to acquire by the end of an educational activity or program.  \\n- They guide both instructors and students by providing a framework for lesson planning, assessment, and evaluation of progress.  \\n- Well-crafted learning objectives are measurable, helping to ensure that educational outcomes can be effectively assessed.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 42, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-BtsBOIvPRTVN6VmQaETN7zmQXd3yo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8b3cab74-621f-43e5-9287-c333f581f3d8-0' usage_metadata={'input_tokens': 42, 'output_tokens': 69, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T08:53:48.495210Z",
     "start_time": "2025-07-16T08:53:46.669753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Step 6: Invoking chain with a string output parser\n",
    "response_2 = chain_with_parser.invoke({\"topic\": \"Learning objectives\"})\n",
    "print(response_2)"
   ],
   "id": "c56218f5809433eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Learning objectives clearly define what students are expected to know, understand, or be able to do by the end of a lesson or course.  \n",
      "- They provide a framework for both instructors and learners, guiding the teaching process and assessing student progress.  \n",
      "- Well-crafted learning objectives enhance engagement and motivation by making goals explicit and measurable.  \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recipe 3.2.2 StructuredOutputParser() ",
   "id": "f48cf2825bfc6664"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:12:17.641028Z",
     "start_time": "2025-07-17T11:12:17.577348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "## Step 1: Define the schema for the field\n",
    "response_schema = [\n",
    "    ## Field 1\n",
    "    ResponseSchema(\n",
    "        name = \"topic\",\n",
    "        description= \"The category of the question, such as admissions, exams, or fees\"\n",
    "),\n",
    "   ## Field 1\n",
    "    ResponseSchema(name=\"confidence\", \n",
    "                   description=\"A  confidence score (from 0 to 1) reflecting certainty about the topic\"\n",
    "                   )\n",
    "]\n",
    "\n",
    "## Step 2: Link the schema to the parser\n",
    "parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    " \n",
    "print(parser.get_format_instructions())\n"
   ],
   "id": "f5aac04cce5af7d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"topic\": string  // The category of the question, such as admissions, exams, or fees\n",
      "\t\"confidence\": string  // A  confidence score (from 0 to 1) reflecting certainty about the topic\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T11:12:24.704972Z",
     "start_time": "2025-07-17T11:12:22.407935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    " \n",
    "## Step 3: Define a prompt with format instructions as a partial variable\n",
    "prompt = PromptTemplate.from_template(\n",
    "    input_variable=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    template=\"\"\"Classify the question and return:\n",
    "    - topic (e.g., admissions, exams, fees)\n",
    "    - confidence (a number between 0 and 1)\n",
    "    {format_instructions}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "## Step 4: Define a model model\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    " \n",
    "## Step 5: Define a chain\n",
    "full_chain = prompt | chat_model  | parser\n",
    " \n",
    "## Step 6: Invoke the chain with a question\n",
    "output = full_chain.invoke({\"question\": \"I urgently need to finalize my child’s admission papers. Who can we speak to?\"})\n",
    " \n",
    "print(output) \n",
    " \n",
    "\n"
   ],
   "id": "cf4a91902a4cbb21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'admissions', 'confidence': 0.9}\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
